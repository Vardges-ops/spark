{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2042b5c3-a216-46ea-a322-9f49fbe238eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83ca9f6-332f-4e23-9f41-72fbf1c32cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(10000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35cfcf9-9bc5-4d7d-a102-335cee8ddc89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Zambia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Cyprus', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Indonesia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Georgia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Namibia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Suriname', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Estonia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Papua New Guinea', count=1),\n",
       " Row(DEST_COUNTRY_NAME='New Caledonia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bulgaria', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bahrain', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Iraq', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Kosovo', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Montenegro', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME=\"Cote d'Ivoire\", ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cyprus', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Burkina Faso', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Lithuania', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Djibouti', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Indonesia', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Vietnam', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Georgia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malta', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Croatia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Hungary', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Liberia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Niger', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Malaysia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Greenland', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Liberia', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hungary', count=3),\n",
       " Row(DEST_COUNTRY_NAME='Singapore', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='Papua New Guinea', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='Thailand', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malaysia', count=3),\n",
       " Row(DEST_COUNTRY_NAME='Tunisia', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='Bulgaria', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greenland', count=4),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Thailand', count=4),\n",
       " Row(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4),\n",
       " Row(DEST_COUNTRY_NAME='French Guiana', ORIGIN_COUNTRY_NAME='United States', count=5),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Paraguay', count=6),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Egypt', count=12),\n",
       " Row(DEST_COUNTRY_NAME='Pakistan', ORIGIN_COUNTRY_NAME='United States', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Pakistan', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Czech Republic', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ethiopia', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bolivia', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Ethiopia', ORIGIN_COUNTRY_NAME='United States', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Angola', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ukraine', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cook Islands', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Czech Republic', ORIGIN_COUNTRY_NAME='United States', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Cook Islands', ORIGIN_COUNTRY_NAME='United States', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Uruguay', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cape Verde', count=14),\n",
       " Row(DEST_COUNTRY_NAME='Ukraine', ORIGIN_COUNTRY_NAME='United States', count=14),\n",
       " Row(DEST_COUNTRY_NAME='Romania', ORIGIN_COUNTRY_NAME='United States', count=14),\n",
       " Row(DEST_COUNTRY_NAME='Morocco', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Latvia', count=15),\n",
       " Row(DEST_COUNTRY_NAME='Angola', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='Ghana', ORIGIN_COUNTRY_NAME='United States', count=18),\n",
       " Row(DEST_COUNTRY_NAME='Bahrain', ORIGIN_COUNTRY_NAME='United States', count=19),\n",
       " Row(DEST_COUNTRY_NAME='Latvia', ORIGIN_COUNTRY_NAME='United States', count=19),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Morocco', count=19),\n",
       " Row(DEST_COUNTRY_NAME='Cape Verde', ORIGIN_COUNTRY_NAME='United States', count=20),\n",
       " Row(DEST_COUNTRY_NAME='Dominica', ORIGIN_COUNTRY_NAME='United States', count=20),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ghana', count=20),\n",
       " Row(DEST_COUNTRY_NAME='Azerbaijan', ORIGIN_COUNTRY_NAME='United States', count=21),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Azerbaijan', count=21),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greece', count=23),\n",
       " Row(DEST_COUNTRY_NAME='Fiji', ORIGIN_COUNTRY_NAME='United States', count=24),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Samoa', count=25),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Fiji', count=25),\n",
       " Row(DEST_COUNTRY_NAME='Samoa', ORIGIN_COUNTRY_NAME='United States', count=25),\n",
       " Row(DEST_COUNTRY_NAME='Finland', ORIGIN_COUNTRY_NAME='United States', count=26),\n",
       " Row(DEST_COUNTRY_NAME='Kiribati', ORIGIN_COUNTRY_NAME='United States', count=26),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominica', count=27),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Finland', count=28),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kuwait', count=28),\n",
       " Row(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30),\n",
       " Row(DEST_COUNTRY_NAME='Bolivia', ORIGIN_COUNTRY_NAME='United States', count=30),\n",
       " Row(DEST_COUNTRY_NAME='Palau', ORIGIN_COUNTRY_NAME='United States', count=30),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Palau', count=31),\n",
       " Row(DEST_COUNTRY_NAME='Poland', ORIGIN_COUNTRY_NAME='United States', count=32),\n",
       " Row(DEST_COUNTRY_NAME='Kuwait', ORIGIN_COUNTRY_NAME='United States', count=32),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Poland', count=33),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Suriname', count=34),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kiribati', count=35),\n",
       " Row(DEST_COUNTRY_NAME='South Africa', ORIGIN_COUNTRY_NAME='United States', count=36),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Anguilla', count=38),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Marshall Islands', count=39)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/Volumes/workspace/myschema/myvolume/2015-summary.csv\")\n",
    "flight_data.sort(\"count\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "flight_data.sort(\"count\").take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd0af6a1-37cd-4411-a001-33526a95b9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_data.createOrReplaceTempView(\"flight_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b07002-860c-4a9b-be86-666ad1f44453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonGroupingAgg(keys=[DEST_COUNTRY_NAME#13459], functions=[finalmerge_count(merge count#13493L) AS count(1)#13490L])\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#9043]\n               +- PhotonShuffleExchangeSink hashpartitioning(DEST_COUNTRY_NAME#13459, 50)\n                  +- PhotonGroupingAgg(keys=[DEST_COUNTRY_NAME#13459], functions=[partial_count(1) AS count#13493L])\n                     +- PhotonRowToColumnar\n                        +- FileScan csv [DEST_COUNTRY_NAME#13459] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/myschema/myvolume/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonGroupingAgg(keys=[DEST_COUNTRY_NAME#13459], functions=[finalmerge_count(merge count#13498L) AS count(1)#13496L])\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#9085]\n               +- PhotonShuffleExchangeSink hashpartitioning(DEST_COUNTRY_NAME#13459, 50)\n                  +- PhotonGroupingAgg(keys=[DEST_COUNTRY_NAME#13459], functions=[partial_count(1) AS count#13498L])\n                     +- PhotonRowToColumnar\n                        +- FileScan csv [DEST_COUNTRY_NAME#13459] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/myschema/myvolume/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "dataFrameWay = flight_data\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc6b6b9-cca5-4fca-bd76-92589b81a02f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData2015 = flight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c233e01a-7202-4c9c-a9fa-d40f26b74329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e88f3a-7a02-4029-a389-9d81158d1193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b1813b-7a50-46f0-b031-96c18fd0bf54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/Volumes/workspace/myschema/myvolume/2010-12-01.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema\n",
    "\n",
    "from pyspark.sql.functions import window, column, desc, col\n",
    "\n",
    "staticDataFrame.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\").groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\").show(5)\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd8f63b-85a4-4b97-87c3-9f45a196e6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5330412972019294>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m streamingDataFrame \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\\\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;241m.\u001B[39mschema(staticSchema)\\\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxFilesPerTrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m)\\\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/checkpoints/retail\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/data/retail-data/by-day/*.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m purchaseByCustomerPerHour \u001B[38;5;241m=\u001B[39m streamingDataFrame\\\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;241m.\u001B[39mselectExpr(\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomerId\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomerId\u001B[39m\u001B[38;5;124m\"\u001B[39m), window(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvoiceDate\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 day\u001B[39m\u001B[38;5;124m\"\u001B[39m))\\\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_cost\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     18\u001B[0m purchaseByCustomerPerHour\u001B[38;5;241m.\u001B[39mwriteStream\\\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_purchases\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;241m.\u001B[39mstart()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'staticSchema' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'staticSchema' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'staticSchema' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN00",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5330412972019294>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m streamingDataFrame \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\\\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;241m.\u001B[39mschema(staticSchema)\\\n\u001B[1;32m      3\u001B[0m \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxFilesPerTrigger\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m)\\\n\u001B[1;32m      4\u001B[0m \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/checkpoints/retail\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      5\u001B[0m \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      6\u001B[0m \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      7\u001B[0m \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/data/retail-data/by-day/*.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m purchaseByCustomerPerHour \u001B[38;5;241m=\u001B[39m streamingDataFrame\\\n\u001B[1;32m     10\u001B[0m \u001B[38;5;241m.\u001B[39mselectExpr(\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomerId\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomerId\u001B[39m\u001B[38;5;124m\"\u001B[39m), window(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvoiceDate\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 day\u001B[39m\u001B[38;5;124m\"\u001B[39m))\\\n\u001B[1;32m     16\u001B[0m \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_cost\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m purchaseByCustomerPerHour\u001B[38;5;241m.\u001B[39mwriteStream\\\n\u001B[1;32m     19\u001B[0m \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     20\u001B[0m \u001B[38;5;241m.\u001B[39mqueryName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_purchases\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     21\u001B[0m \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m     22\u001B[0m \u001B[38;5;241m.\u001B[39mstart()\n",
        "\u001B[0;31mNameError\u001B[0m: name 'staticSchema' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    ".schema(staticSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".option(\"checkpointLocation\", \"/tmp/checkpoints/retail\")\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(\"/data/retail-data/by-day/*.csv\")\n",
    "\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")\n",
    "\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828514aa-d3c7-4e54-be96-9c6fb805d38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField('DEST_COUNTRY_NAME', StringType(), True), \n",
    "     StructField('ORIGIN_COUNTRY_NAME', StringType(), True), \n",
    "     StructField('count', LongType(), True)]\n",
    "    )\n",
    "df = spark.read.format(\"json\").schema(schema)\\\n",
    "    .load(\"/Volumes/workspace/myschema/myvolume/2015-summary.json\")\\\n",
    "        .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b843c53-c0e8-49b5-a1e7-4001d3af73c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n+-------------+\n|  destination|\n+-------------+\n|United States|\n|United States|\n+-------------+\nonly showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a9f98b-015a-4611-92ea-714b3bd2ac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------------+-------------+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count_origin|withinCountry|count_doubled|\n+-----------------+-------------------+------------+-------------+-------------+\n|    United States|            Romania|          15|        false|           30|\n|    United States|            Croatia|           1|        false|            2|\n+-----------------+-------------------+------------+-------------+-------------+\nonly showing top 2 rows\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.selectExpr(\n",
    "# \"*\", # all original columns\n",
    "# \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    "# .show(2)\n",
    "\n",
    "# df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "# from pyspark.sql.functions import lit\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "# \"*\", # all original columns\n",
    "# \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    "# .show(2)\n",
    "\n",
    "# df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "# from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "new_df = df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
    "    .withColumn(\"count_doubled\", expr(\"count * 2\"))\\\n",
    "        .withColumnRenamed(\"count\", \"count_origin\")\n",
    "new_df.show(2)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# new_df.filter(col(\"count_doubled\") > 200).filter(\"DEST_COUNTRY_NAME = 'United States'\").show(2)\n",
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1195c25a-0fff-4212-acc3-415c087164d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----+\n| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+------------------+-------------------+-----+\n|            Canada|      United States| 8399|\n|            Mexico|      United States| 7140|\n|    United Kingdom|      United States| 2025|\n|             Japan|      United States| 1548|\n|           Germany|      United States| 1468|\n|Dominican Republic|      United States| 1353|\n|       South Korea|      United States| 1048|\n|       The Bahamas|      United States|  955|\n|            France|      United States|  935|\n|          Colombia|      United States|  873|\n|            Brazil|      United States|  853|\n|       Netherlands|      United States|  776|\n|             China|      United States|  772|\n|           Jamaica|      United States|  666|\n|        Costa Rica|      United States|  588|\n|       El Salvador|      United States|  561|\n|            Panama|      United States|  510|\n|              Cuba|      United States|  466|\n|             Spain|      United States|  420|\n|         Guatemala|      United States|  397|\n+------------------+-------------------+-----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "schemaa = df.schema\n",
    "newRows = [\n",
    "Row(\"New Country\", \"Other Country\", 5),\n",
    "Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "newDF = spark.createDataFrame(newRows, schema=schemaa)\n",
    "df.union(newDF)\\\n",
    "    .where(\"count>1\")\\\n",
    "        .where(col(\"DEST_COUNTRY_NAME\") != \"United States\")\\\n",
    "            .sort(col(\"count\").desc(), col(\"ORIGIN_COUNTRY_NAME\").asc())\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e2eb3b-0960-49d4-8e92-db035d0da592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonShuffleExchangeSource\n         +- PhotonShuffleMapStage REPARTITION_BY_NUM, [id=#9273]\n            +- PhotonShuffleExchangeSink hashpartitioning(DEST_COUNTRY_NAME#13164, 5)\n               +- PhotonJsonScan json [DEST_COUNTRY_NAME#13164,ORIGIN_COUNTRY_NAME#13165,count#13166L] Batched: true, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/workspace/myschema/myvolume/2015-summary.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n|    United States|            Grenada|   62|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|United States    |Romania            |15   |\n|United States    |Croatia            |1    |\n|United States    |Ireland            |344  |\n|Egypt            |United States      |15   |\n|United States    |India              |62   |\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).explain()\n",
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()\n",
    "\n",
    "# collectDF.toLocalIterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9798b889-824e-4b68-be0b-ea5865e51060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_project_real_time",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}