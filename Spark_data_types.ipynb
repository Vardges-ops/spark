{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0074a6d4-c588-4a9b-9900-aaa31a6e6bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+spark_ml_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"demotypes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d8b75b-edfd-44ee-8db6-e6017dd9e9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "            .load(\"/Volumes/workspace/myschema/myvolume/retail_2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ece6f5-ad25-4ae7-a0f6-2416e3fd8c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr, col\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79e1763-30a8-41e1-9e1e-ce2e7949215f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------------+\n|   Description|UnitPrice|round(UnitPrice, 0)|\n+--------------+---------+-------------------+\n|DOTCOM POSTAGE|   569.77|              570.0|\n|DOTCOM POSTAGE|   607.49|              607.0|\n+--------------+---------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, round\n",
    "\n",
    "df.withColumn(\"IsExpensive\", expr(\"UnitPrice > 250\")).filter(\"IsExpensive\").select(\"Description\", \"UnitPrice\", round(df.UnitPrice)).show(5)\n",
    "df.where(col(\"Description\").eqNullSafe(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba083a80-315d-4be3-b3b0-c1921fb960c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+----------+----------+-----+\n|        ltrim|         rtrim|      lpad|      rpad| trim|\n+-------------+--------------+----------+----------+-----+\n|HELLO        |         HELLO|     HELLO|HELLO     |HELLO|\n|HELLO        |         HELLO|     HELLO|HELLO     |HELLO|\n|HELLO        |         HELLO|     HELLO|HELLO     |HELLO|\n+-------------+--------------+----------+----------+-----+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "# df.select(monotonically_increasing_id()).show(5)\n",
    "df.select(\n",
    "    ltrim(lit(\"         HELLO        \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"         HELLO        \")).alias(\"rtrim\"),\n",
    "    lpad(lit(\"HELLO\"), 10, \" \").alias(\"lpad\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rpad\"),\n",
    "    trim(lit(\"         HELLO        \")).alias(\"trim\")\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fddd21-3871-46a8-a41c-72b09326bfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n|date_add(today, 5)|date_sub(today, 5)|\n+------------------+------------------+\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n|        2025-12-30|        2025-12-20|\n+------------------+------------------+\n\n+-------------------------------+\n|datediff(today, SeveralDaysAgo)|\n+-------------------------------+\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n|                             18|\n+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, current_date, current_timestamp, date_add, date_sub, datediff\n",
    "\n",
    "dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "\n",
    "dateDF.select(date_add(dateDF.today, 5), date_sub(dateDF.today, 5)).show()\n",
    "dateDF.withColumn(\"SeveralDaysAgo\", date_sub(dateDF.today, 18)).select(datediff(col(\"today\"), col(\"SeveralDaysAgo\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbbb797-1aca-44ba-8fc5-3d1536186c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>timestamp</th></tr></thead><tbody><tr><td>2020-11-20</td><td>2020-10-07T18:44:12.000Z</td></tr><tr><td>2020-11-20</td><td>2020-10-07T18:44:12.000Z</td></tr><tr><td>2020-11-20</td><td>2020-10-07T18:44:12.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2020-11-20",
         "2020-10-07T18:44:12.000Z"
        ],
        [
         "2020-11-20",
         "2020-10-07T18:44:12.000Z"
        ],
        [
         "2020-11-20",
         "2020-10-07T18:44:12.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, lit, col\n",
    "\n",
    "date_format = \"yyy-dd-MM\"\n",
    "timestamp_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "dateDF = spark.range(3).select(\n",
    "    to_date(lit(\"2020-20-11\"), date_format).alias(\"date\"),\n",
    "    to_timestamp(lit(\"2020-10-07 18:44:12\"), timestamp_format).alias(\"timestamp\")\n",
    ")\n",
    "dateDF.filter(col(\"date\") > lit(\"2018-10-11\")).display()\n",
    "\n",
    "dateDF.na.drop(\"all\", subset=[\"date\", \"timestamp\"])\n",
    "df.na.fill({\"time\":to_date(lit(\"1999-01-01\")), \"timestamp\":to_timestamp(lit(\"1999-01-01 00:00:00\"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b639853-517a-43c1-ba5e-f15d9296a46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n|         Description|complex.InvoiceNo|\n+--------------------+-----------------+\n|WHITE HANGING HEA...|           536365|\n| WHITE METAL LANTERN|           536365|\n+--------------------+-----------------+\nonly showing top 2 rows\n+--------------+\n|split_descr[0]|\n+--------------+\n|         WHITE|\n|         WHITE|\n+--------------+\nonly showing top 2 rows\n+-----------------+\n|size(split_descr)|\n+-----------------+\n|                5|\n|                3|\n+-----------------+\nonly showing top 2 rows\n+--------------------+--------------------+--------+\n|         Description|         split_descr|exploded|\n+--------------------+--------------------+--------+\n|WHITE HANGING HEA...|[WHITE, HANGING, ...|   WHITE|\n|WHITE HANGING HEA...|[WHITE, HANGING, ...| HANGING|\n|WHITE HANGING HEA...|[WHITE, HANGING, ...|   HEART|\n+--------------------+--------------------+--------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col, split, size, explode\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexTable\")\n",
    "complexDF.select(col(\"complex.Description\"), col(\"complex\").getField(\"InvoiceNo\")).show(2)\n",
    "\n",
    "split_df = df.withColumn(\"split_descr\", split(col(\"Description\"), \" \"))\n",
    "split_df.selectExpr(\"split_descr[0]\").show(2)\n",
    "split_df.select(size(col(\"split_descr\"))).show(2)\n",
    "\n",
    "split_df.select(col(\"Description\"), col(\"split_descr\")).withColumn(\"exploded\", explode(col(\"split_descr\"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6196884-b540-4f6f-9a82-4061d473311a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n|complex_map[WHITE METAL LANTERN]|\n+--------------------------------+\n|                            NULL|\n|                            NULL|\n+--------------------------------+\nonly showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, col\n",
    "\n",
    "df.select(create_map(col(\"InvoiceNo\"), col(\"Description\")).alias(\"complex_map\")).selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83756223-2bf0-4dc6-ba90-8e23706febd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|column|                  c0|\n+------+--------------------+\n|     2|{\"myJSONValue\":[1...|\n+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c044e78d-7d00-4d30-8078-7c71df27368c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|             newJSON|              parsed|\n+--------------------+--------------------+\n|{536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n|{536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n|{536365, CREAM CU...|{\"InvoiceNo\":\"536...|\n|{536365, KNITTED ...|{\"InvoiceNo\":\"536...|\n+--------------------+--------------------+\nonly showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, to_json, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType((\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "    ))\n",
    "df_struct = df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    "df_json = df_struct.select(\n",
    "    to_json(col(\"myStruct\")).alias(\"parsed\")\n",
    ")\n",
    "result_df = df_json.select(\n",
    "    from_json(col(\"parsed\"), json_schema).alias(\"newJSON\"),\n",
    "    col(\"parsed\")\n",
    ")\n",
    "result_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84fd318-ef4e-4f50-8b5f-958ce7e3d1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+\n|customer_id|total_amount|spending_tier|\n+-----------+------------+-------------+\n|          1|        45.0|          LOW|\n|          2|       220.0|       MEDIUM|\n|          3|       890.0|         HIGH|\n|          4|        NULL|      UNKNOWN|\n+-----------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "def spending_tier(total_amount):\n",
    "    try:\n",
    "        if total_amount is None:\n",
    "            return \"UNKNOWN\"\n",
    "        elif total_amount < 100:\n",
    "            return \"LOW\"\n",
    "        elif total_amount < 500:\n",
    "            return \"MEDIUM\"\n",
    "        else:\n",
    "            return \"HIGH\"\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spending_tier_udf = udf(spending_tier, StringType())\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 45.0), (2, 220.0), (3, 890.0), (4, None)],\n",
    "    [\"customer_id\", \"total_amount\"]\n",
    ")\n",
    "df.withColumn(\n",
    "    \"spending_tier\",\n",
    "    spending_tier_udf(col(\"total_amount\"))\n",
    ").show()\n",
    "\n",
    "spark.udf.register(\"spending_tier\", spending_tier, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba632d6-012c-402a-bd1a-f47a100c206d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_data_types",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}